{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "from MultiArmedBandit import Arm, bernoulliArm, evolvingBernoulliArm\n",
    "from Exp3 import exp3_Bianchi, exp3P_Bianchi, exp3_IX\n",
    "from OtherBanditAlgorithms import UCB1, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian_traj(T):\n",
    "    \"\"\"gaussian trajectories normalized to lie in [0, 1]\"\"\"\n",
    "    gp = np.cumsum(np.random.normal(0, 2, T))\n",
    "\n",
    "    # normalization (to have rewards in [0, 1]) :\n",
    "    gp += np.abs(min(gp))\n",
    "    gp /= max(gp)\n",
    "    return(gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T = 30000\n",
    "\n",
    "arm1 = Arm(gaussian_traj(T))\n",
    "arm2 = Arm(gaussian_traj(T))\n",
    "arm3 = Arm(gaussian_traj(T))\n",
    "\n",
    "MAB = [arm1, arm2, arm3]\n",
    "\n",
    "plt.plot(arm1.rewards)\n",
    "plt.plot(arm2.rewards)\n",
    "plt.plot(arm3.rewards)\n",
    "plt.legend(['Arm 1', 'Arm 2', 'Arm 3'])\n",
    "plt.title('Rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute best arm at round t\n",
    "arm1_cumsum = np.cumsum(arm1.rewards)\n",
    "arm2_cumsum = np.cumsum(arm2.rewards)\n",
    "arm3_cumsum = np.cumsum(arm3.rewards)\n",
    "\n",
    "best_action_rew = np.zeros(T)\n",
    "\n",
    "for t in range(T):\n",
    "    best_action_rew[t] = max(arm1_cumsum[t], arm2_cumsum[t], arm3_cumsum[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Weak regret bound of Exp3 if the number of rounds is known"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = len(MAB)\n",
    "eta = np.sqrt(2 * np.log(K) / T * K)\n",
    "etas = [eta for _ in range(T)]\n",
    "\n",
    "rew_exp3 = np.zeros(T)\n",
    "n_iter = 20\n",
    "\n",
    "for i in range(n_iter):\n",
    "    rew, _ = exp3_Bianchi(MAB, T, etas)\n",
    "    rew_exp3 += rew\n",
    "    \n",
    "rew_exp3 /= n_iter\n",
    "    \n",
    "exp3_cumsum = np.cumsum(rew_exp3)\n",
    "plt.plot(exp3_cumsum - best_action_rew, '--o', markevery=1500, label=\"Exp3\")\n",
    "\n",
    "plt.axhline(np.sqrt( 2 * T * K * np.log(K)), label=\"weak regret bound\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Weak regret of Exp3 when the number of rounds is uknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = len(MAB)\n",
    "etas = [np.sqrt(2 * np.log(K) / (t + 1) * K) for t in range(T)]\n",
    "\n",
    "rew_exp3 = np.zeros(T)\n",
    "n_iter = 1000\n",
    "\n",
    "for i in range(n_iter):\n",
    "    rew_exp3 += exp3_Bianchi(MAB, T, etas)\n",
    "rew_exp3 /= n_iter\n",
    "\n",
    "    \n",
    "exp3_cumsum = np.cumsum(rew_exp3)\n",
    "plt.plot(exp3_cumsum - best_action_rew, '--o', markevery=1500, label=\"Exp3\")\n",
    "\n",
    "plt.plot([np.sqrt(4 * t * K * np.log(K)) for t in range(T)], label=\"weak regret bound\")\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Regret of Exp3.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = len(MAB)\n",
    "delta = 0.05\n",
    "eta = 0.95 * np.sqrt(np.log(K) / (T * K))\n",
    "gamma = 1.05 * np.sqrt(np.log(K) * K / T)\n",
    "beta = np.sqrt(np.log(K / delta) /  (T * K))\n",
    "\n",
    "#calculate best arm at each round\n",
    "best_action_rew = np.zeros(T)\n",
    "for t in range(T):\n",
    "    best_action_rew[t] = max(arm1_cumsum[t], arm2_cumsum[t], arm3_cumsum[t])\n",
    "\n",
    "#first type of regret bound\n",
    "rew_exp3P = exp3P_Bianchi(MAB, T, eta, gamma, beta)\n",
    "\n",
    "exp3P_cumsum = np.cumsum(rew_exp3P)\n",
    "\n",
    "plt.plot(exp3P_cumsum - best_action_rew, label=\"Exp3P 1\") \n",
    "plt.axhline(5.15 * np.sqrt(T * K * np.log(K / delta)), c=\"black\", label=\"Regret 1 bound\")\n",
    "\n",
    "#second type of regret bound\n",
    "beta = np.sqrt(np.log(K) /  (T * K))\n",
    "\n",
    "rew_exp3P = exp3P_Bianchi(MAB, T, eta, gamma, beta)\n",
    "exp3P_cumsum = np.cumsum(rew_exp3P)\n",
    "plt.plot(exp3P_cumsum - best_action_rew, label=\"Exp3P 2\") \n",
    "plt.axhline(5.15 * np.sqrt(T * K * np.log(K)) + np.sqrt(T * K / np.log(K)) * np.log(1 / delta), c=\"red\", label=\"Regret 2 bound\")\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Regret of Exp3.P and its bounds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Weak regret of Exp3.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "eta = 0.95 * np.sqrt(np.log(K) / (T * K))\n",
    "gamma = 1.05 * np.sqrt(np.log(K) * K / T)\n",
    "beta = np.sqrt(np.log(K) /  (T * K))\n",
    "\n",
    "#calculate best arm at each round\n",
    "best_action_rew = np.zeros(T)\n",
    "for t in range(T):\n",
    "    best_action_rew[t] = max(arm1_cumsum[t], arm2_cumsum[t], arm3_cumsum[t])\n",
    "\n",
    "rew_exp3P = np.zeros(T)\n",
    "n_iter = 100\n",
    "for i in range(n_iter):\n",
    "    rew_exp3P += exp3P_Bianchi(MAB, T, eta, gamma, beta)\n",
    "rew_exp3P /= n_iter\n",
    "\n",
    "exp3P_cumsum = np.cumsum(rew_exp3P)\n",
    "\n",
    "plt.plot(exp3P_cumsum - best_action_rew, label=\"Exp3P 2\") \n",
    "plt.axhline(5.15 * np.sqrt(T * K * np.log(K)) + np.sqrt(T * K / np.log(K)), c=\"red\", label=\"Weak regret bound\")\n",
    "\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Compairison of different algorithms on gaussian arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set up the parameters\n",
    "\n",
    "K = len(MAB)\n",
    "eta = np.sqrt(2 * np.log(K) / T * K)\n",
    "etas_exp3 = [eta for _ in range(T)]\n",
    "\n",
    "delta = 0.05\n",
    "eta = 0.95 * np.sqrt(np.log(K) / (T * K))\n",
    "gamma = 1.05 * np.sqrt(np.log(K) * K / T)\n",
    "beta = np.sqrt(np.log(K / delta) /  (T * K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rew = exp3_Bianchi(MAB, T, eta=etas)\n",
    "rew_P = exp3P_Bianchi(MAB, T, beta=beta, gamma=gamma, eta=eta)\n",
    "rew_IX, _ = exp3_IX(MAB, T, eta=eta, gamma=0.5)\n",
    "rew_UCB, _ = UCB1(MAB, T, rho=0.2)\n",
    "rew_random = Random(MAB, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arm1_cumsum = np.cumsum(arm1.rewards)\n",
    "arm2_cumsum = np.cumsum(arm2.rewards)\n",
    "arm3_cumsum = np.cumsum(arm3.rewards)\n",
    "exp3_cumsum = np.cumsum(rew)\n",
    "exp3_P_cumsum = np.cumsum(rew_P)\n",
    "exp3_IX_cumsum = np.cumsum(rew_IX)\n",
    "UCB_cumsum = np.cumsum(rew_UCB)\n",
    "rew_cumsum = np.cumsum(rew_random)\n",
    "\n",
    "plt.plot(arm1_cumsum, alpha=0.5, label=\"Arm 1\")\n",
    "plt.plot(arm2_cumsum, alpha=0.5, label=\"Arm 2\")\n",
    "plt.plot(arm3_cumsum, alpha=0.5, label=\"Arm 3\")\n",
    "\n",
    "plt.plot(exp3_cumsum, '--o', markevery=1500, label=\"Exp3\")\n",
    "\n",
    "plt.plot(exp3_P_cumsum, '--o', markevery=1500, label=\"Exp3.P\")\n",
    "plt.plot(exp3_IX_cumsum, '--o', markevery=1500, label=\"Exp3-IX\")\n",
    "plt.plot(UCB_cumsum, '--o', markevery=1500, label=\"UCB\")\n",
    "plt.plot(rew_cumsum, '--o', markevery=1500, label=\"random\")\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title(\"Cumulative reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
